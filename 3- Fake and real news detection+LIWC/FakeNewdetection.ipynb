{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"FakeNewdetection.ipynb","provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"i3asi4F7Q6gh","executionInfo":{"status":"ok","timestamp":1620677773541,"user_tz":240,"elapsed":596,"user":{"displayName":"Jaber Valinejad","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gip2ZmJmxo_rPu4BNV6ze8B_Fjb2gQ7h8I-FHgs=s64","userId":"11093588303192782745"}},"outputId":"ea7252bb-203e-486d-8c19-eab5509cffe5"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"D5q-8M0NQzry","executionInfo":{"status":"ok","timestamp":1620677775363,"user_tz":240,"elapsed":864,"user":{"displayName":"Jaber Valinejad","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gip2ZmJmxo_rPu4BNV6ze8B_Fjb2gQ7h8I-FHgs=s64","userId":"11093588303192782745"}}},"source":["import numpy as np\n","import pandas as pd\n","import os, sys\n","import matplotlib.mlab as mlab\n","import matplotlib.pyplot as plt\n","from sklearn import preprocessing\n","le = preprocessing.LabelEncoder()\n","import itertools\n","from sklearn.model_selection import train_test_split\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.linear_model import PassiveAggressiveClassifier\n","from sklearn.metrics import accuracy_score, confusion_matrix\n"],"execution_count":3,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vcnotAViRDXz"},"source":["# Reading Tweeter Data"]},{"cell_type":"code","metadata":{"id":"a7ZW12-IQsOg","executionInfo":{"status":"ok","timestamp":1620677809806,"user_tz":240,"elapsed":9237,"user":{"displayName":"Jaber Valinejad","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gip2ZmJmxo_rPu4BNV6ze8B_Fjb2gQ7h8I-FHgs=s64","userId":"11093588303192782745"}}},"source":["dt_Australia= pd.read_csv('/content/drive/Shareddrives/MY Files/W IEEEmsc:resilience+fake news+Web scrapping/1-  Tweets modifiers/Country Tweetsss/Australia_tweet-ids.csv')\n","dt_Brazil= pd.read_csv('/content/drive/Shareddrives/MY Files/W IEEEmsc:resilience+fake news+Web scrapping/1-  Tweets modifiers/Country Tweetsss/Brazil_tweet-ids.csv')\n","dt_China = pd.read_csv('/content/drive/Shareddrives/MY Files/W IEEEmsc:resilience+fake news+Web scrapping/1-  Tweets modifiers/Country Tweetsss/China_tweet-ids.csv')\n","dt_Iran = pd.read_csv('/content/drive/Shareddrives/MY Files/W IEEEmsc:resilience+fake news+Web scrapping/1-  Tweets modifiers/Country Tweetsss/Iran_tweet-ids.csv')\n","dt_Italy = pd.read_csv('/content/drive/Shareddrives/MY Files/W IEEEmsc:resilience+fake news+Web scrapping/1-  Tweets modifiers/Country Tweetsss/Italy_tweet-ids.csv')\n","dt_Singapour = pd.read_csv('/content/drive/Shareddrives/MY Files/W IEEEmsc:resilience+fake news+Web scrapping/1-  Tweets modifiers/Country Tweetsss/Singapore_tweet-ids.csv')\n","dt_South_Korea= pd.read_csv('/content/drive/Shareddrives/MY Files/W IEEEmsc:resilience+fake news+Web scrapping/1-  Tweets modifiers/Country Tweetsss/SouthKorea_tweet-ids.csv')\n","dt_UK= pd.read_csv('/content/drive/Shareddrives/MY Files/W IEEEmsc:resilience+fake news+Web scrapping/1-  Tweets modifiers/Country Tweetsss/UK_tweet-ids.csv')\n","dt_USA= pd.read_csv('/content/drive/Shareddrives/MY Files/W IEEEmsc:resilience+fake news+Web scrapping/1-  Tweets modifiers/Country Tweetsss/US_tweet-ids.csv')\n","Countries=[dt_Australia,dt_Singapour,dt_South_Korea,dt_UK,dt_USA]"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"SA75sKicR4-5","executionInfo":{"status":"ok","timestamp":1620677815930,"user_tz":240,"elapsed":183,"user":{"displayName":"Jaber Valinejad","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gip2ZmJmxo_rPu4BNV6ze8B_Fjb2gQ7h8I-FHgs=s64","userId":"11093588303192782745"}}},"source":["import time\n","def Reading(Data): \n","  data_tweet = pd.DataFrame()\n","  data_tweet['Time']= Data['created_at']\n","  data_tweet['text']= Data['text']\n","  data_tweet['user_followers_count']= Data['user_followers_count']\n","  data_tweet['user_followee_count']= Data['user_friends_count']\n","  data_tweet['lang']= Data['lang']\n","  return data_tweet\n","\n","\n","\n","def date_correcting(data_tweet):\n","  OK=data_tweet['Time']\n","  df = pd.DataFrame()\n","  df['Time']=''\n","  for i in range(OK.shape[0]):\n","     X=OK[i]\n","     #Y= time.strftime('%Y%m%d%H', time.strptime(X,'%a %b %d %H:%M:%S +0000 %Y'))\n","     Y= time.strftime('%Y%m', time.strptime(X,'%a %b %d %H:%M:%S +0000 %Y'))\n","     df.at[i,'Time']=pd.to_numeric(Y)\n","\n","  data_tweet.drop(['Time'], axis=1, inplace=True)\n","  data_tweet['time']=df['Time']\n","  data_tweet=data_tweet.sort_values(by=['time'])\n","  data_tweet2=data_tweet.copy()\n","  data_tweet.index=range(data_tweet.shape[0])\n","  return data_tweet\n","\n","def en_correcting(data_tweet):\n","    a_list =[]\n","    for i in range(data_tweet.shape[0]):\n","      if data_tweet['lang'][i] != 'en':\n","          a_list.append(i) \n","    data_tweet = data_tweet.drop(labels=a_list, axis=0)      \n","    data_tweet.index=range(data_tweet.shape[0])\n","    return  data_tweet\n","\n","def process(Data):\n","  Data=Reading(Data)\n","  if Data.shape[0] >=80000:\n","     Data=Data.sample(n=80000)\n","  Data.index=range(Data.shape[0])\n","  Data=en_correcting(Data) \n","  Data=Data.sample(n=43000)\n","  Data.index=range(Data.shape[0])\n","  Data=date_correcting(Data)\n","  return Data"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"YowINpECXv7g","executionInfo":{"status":"ok","timestamp":1620678278855,"user_tz":240,"elapsed":458359,"user":{"displayName":"Jaber Valinejad","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gip2ZmJmxo_rPu4BNV6ze8B_Fjb2gQ7h8I-FHgs=s64","userId":"11093588303192782745"}}},"source":["Countries=[dt_Australia,dt_Singapour,dt_South_Korea,dt_UK,dt_USA]\n","for i in range(len(Countries)):  \n","   Countries[i]=process(Countries[i])"],"execution_count":8,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_W3HY_z_Qsrd"},"source":["# Fake news detection"]},{"cell_type":"markdown","metadata":{"id":"rKlHnUipf62F"},"source":["# Approach 2 ( Data Set 2):\n"," There are two files, one for real news and one for fake news (both in English) with a total of 23481 “fake” tweets and 21417 “real” articles.\n","\n","https://www.kaggle.com/clmentbisaillon/fake-and-real-news-dataset"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":243},"id":"-vQOPopzgsQz","executionInfo":{"status":"ok","timestamp":1620678367404,"user_tz":240,"elapsed":56066,"user":{"displayName":"Jaber Valinejad","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gip2ZmJmxo_rPu4BNV6ze8B_Fjb2gQ7h8I-FHgs=s64","userId":"11093588303192782745"}},"outputId":"e1c02db4-2eb7-4a9b-811e-d08f0f98e4d8"},"source":["fake = pd.read_csv(\"/content/drive/Shareddrives/MY Files/W IEEEmsc:resilience+fake news+Web scrapping/3- Fake and real news detections+LIWC/Data_set 2/Fake.csv\")\n","true = pd.read_csv(\"/content/drive/Shareddrives/MY Files/W IEEEmsc:resilience+fake news+Web scrapping/3- Fake and real news detections+LIWC/Data_set 2/True.csv\")\n","fake['target'] = 'FAKE'\n","true['target'] = 'REAL'\n","data = pd.concat([fake, true]).reset_index(drop = True)\n","from sklearn.utils import shuffle\n","data = shuffle(data)\n","data = data.reset_index(drop=True)\n","\n","data.drop([\"date\"],axis=1,inplace=True)\n","data.drop([\"title\"],axis=1,inplace=True)\n","#lower case\n","data['text'] = data['text'].apply(lambda x: x.lower())\n","#remove punctuation\n","import string\n","def punctuation_removal(text):\n","    all_list = [char for char in text if char not in string.punctuation]\n","    clean_str = ''.join(all_list)\n","    return clean_str\n","\n","data['text'] = data['text'].apply(punctuation_removal)\n","\n","#Remove stopwords:\n","import nltk\n","nltk.download('stopwords')\n","from nltk.corpus import stopwords\n","stop = stopwords.words('english')\n","data['text'] = data['text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n","x_train,x_test,y_train,y_test = train_test_split(data['text'], data.target, test_size=0.2, random_state=42)\n","data.head()"],"execution_count":9,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>text</th>\n","      <th>subject</th>\n","      <th>target</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>donald trump administration white house leakie...</td>\n","      <td>News</td>\n","      <td>FAKE</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>washington reuters white house spokesman sean ...</td>\n","      <td>politicsNews</td>\n","      <td>REAL</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>almost 20 years ago david bowie released music...</td>\n","      <td>News</td>\n","      <td>FAKE</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>brussels reuters european union agreed financi...</td>\n","      <td>worldnews</td>\n","      <td>REAL</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>edinburgh reuters scotland first minister nico...</td>\n","      <td>worldnews</td>\n","      <td>REAL</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                                text       subject target\n","0  donald trump administration white house leakie...          News   FAKE\n","1  washington reuters white house spokesman sean ...  politicsNews   REAL\n","2  almost 20 years ago david bowie released music...          News   FAKE\n","3  brussels reuters european union agreed financi...     worldnews   REAL\n","4  edinburgh reuters scotland first minister nico...     worldnews   REAL"]},"metadata":{"tags":[]},"execution_count":9}]},{"cell_type":"code","metadata":{"id":"fUpb3_CxTcL5","executionInfo":{"status":"ok","timestamp":1620678702654,"user_tz":240,"elapsed":13003,"user":{"displayName":"Jaber Valinejad","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gip2ZmJmxo_rPu4BNV6ze8B_Fjb2gQ7h8I-FHgs=s64","userId":"11093588303192782745"}}},"source":["#Initialize a TfidfVectorizer\n","tfidf_vectorizer=TfidfVectorizer(stop_words='english', max_df=0.7)\n","#Fit and transform train set, transform test set\n","tfidf_train=tfidf_vectorizer.fit_transform(x_train) \n","tfidf_test=tfidf_vectorizer.transform(x_test)"],"execution_count":12,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HlqcKnQxTqdj"},"source":[" PassiveAggressiveClassifier?\n","\n","Passive Aggressive algorithms are online learning algorithms. Such an algorithm remains passive for a correct classification outcome, and turns aggressive in the event of a miscalculation, updating and adjusting. Unlike most other algorithms, it does not converge. Its purpose is to make updates that correct the loss, causing very little change in the norm of the weight vector.\n"]},{"cell_type":"markdown","metadata":{"id":"X8Iz3fhRtTte"},"source":["*Other Classifiers*"]},{"cell_type":"code","metadata":{"id":"clT3NtB4ZOgV","executionInfo":{"status":"ok","timestamp":1620678724458,"user_tz":240,"elapsed":164,"user":{"displayName":"Jaber Valinejad","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gip2ZmJmxo_rPu4BNV6ze8B_Fjb2gQ7h8I-FHgs=s64","userId":"11093588303192782745"}}},"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","from matplotlib.colors import ListedColormap\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.datasets import make_moons, make_circles, make_classification\n","from sklearn.neighbors import KNeighborsRegressor\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.linear_model import PassiveAggressiveClassifier\n","from sklearn.neural_network import MLPClassifier\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.svm import SVC\n","from sklearn.gaussian_process import GaussianProcessClassifier\n","from sklearn.gaussian_process.kernels import RBF\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n","from sklearn.naive_bayes import GaussianNB\n","from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n","from sklearn import metrics\n","from sklearn.metrics import precision_recall_fscore_support\n","#part 2 \n","# Bagging meta-estimator #\n","from sklearn.ensemble import BaggingClassifier\n","from sklearn.neighbors import KNeighborsClassifier"],"execution_count":13,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ld6IfIbTs_At","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1620680183170,"user_tz":240,"elapsed":971590,"user":{"displayName":"Jaber Valinejad","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gip2ZmJmxo_rPu4BNV6ze8B_Fjb2gQ7h8I-FHgs=s64","userId":"11093588303192782745"}},"outputId":"cf7c944f-7c7f-490f-fc67-0d0d4f27c2ee"},"source":["classifiers = [#PassiveAggressiveClassifier(max_iter=50), \n","               #KNeighborsRegressor(n_neighbors=10) , \n","               #LogisticRegression(random_state=0,max_iter=100000),           \n","   #BaggingClassifier(KNeighborsClassifier(),max_samples=0.5, max_features=0.5),\n","    #KNeighborsClassifier(3),\n","    #SVC(kernel=\"linear\", C=0.025),\n","    #SVC(gamma=2, C=1),\n","   #GaussianProcessClassifier(1.0 * RBF(1.0)),\n","    #DecisionTreeClassifier(max_depth=5),\n","    #RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),\n","    MLPClassifier(alpha=1, max_iter=1000),\n","    #AdaBoostClassifier(),\n","    #GaussianNB(),\n","    #QuadraticDiscriminantAnalysis()\n","    ]\n","\n","classifiers_name = [ #'PassiveAggressiveClassifier(max_iter=50)',  \n","                    #'KNeighborsRegressor(n_neighbors=10)' ,   \n","                    #'logistic regression',\n","     #'BaggingClassifier(KNeighborsClassifier(),max_samples=0.5, max_features=0.5',             \n","    #'KNeighborsClassifier(3)',\n","    #'SVC(kernel=\"linear\", C=0.025)',\n","    #'SVC(gamma=2, C=1)',\n","    #'GaussianProcessClassifier(1.0 * RBF(1.0))',\n","    #'DecisionTreeClassifier(max_depth=5)',\n","    #'RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1)',\n","    ' Multi Layer Perceptron: MLPClassifier(alpha=1, max_iter=1000)',\n","    #'AdaBoostClassifier()',\n","    #'GaussianNB()/ Naive Bayes',\n","    #'QuadraticDiscriminantAnalysis()'\n","    ]\n","for i in range(0,len(classifiers)):\n"," print( classifiers_name[i])\n"," classifier=classifiers[i]\n"," classifier.fit(tfidf_train,y_train)\n"," y_pred = classifier.predict(tfidf_test)\n"," accuracy_met=metrics.accuracy_score(y_pred,y_test)\n"," print(\"Test Accuracy\",accuracy_met)\n"," (precision, recall, fscore, support) = precision_recall_fscore_support(y_test, y_pred, average='macro')\n"," print('Precision: {}\\tRecall: {}\\tfscore:{}'.format(precision, recall, fscore))\n"," print('confusion_matrix:',confusion_matrix(y_test,y_pred, labels=['FAKE','REAL']))\n"," print('\\n ')\n"],"execution_count":23,"outputs":[{"output_type":"stream","text":[" Multi Layer Perceptron: MLPClassifier(alpha=1, max_iter=1000)\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:573: UserWarning: Training interrupted by user.\n","  warnings.warn(\"Training interrupted by user.\")\n"],"name":"stderr"},{"output_type":"stream","text":["Test Accuracy 0.966369710467706\n","Precision: 0.9668706512820747\tRecall: 0.9659245350500716\tfscore:0.9662909745338478\n","confusion_matrix: [[4556  104]\n"," [ 198 4122]]\n","\n"," \n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"s5trCCaGYYL-","executionInfo":{"status":"ok","timestamp":1618104505179,"user_tz":240,"elapsed":63460,"user":{"displayName":"Jaber Valinejad","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gip2ZmJmxo_rPu4BNV6ze8B_Fjb2gQ7h8I-FHgs=s64","userId":"11093588303192782745"}},"outputId":"af099740-4cc5-475c-e508-ce4daf3a7c5f"},"source":["classifier_PAC=PassiveAggressiveClassifier(max_iter=50) \n","classifier_PAC.fit(tfidf_train,y_train)\n","classifier_LR=LogisticRegression(random_state=0,max_iter=100000)\n","classifier_LR.fit(tfidf_train,y_train)\n","classifier_DT=DecisionTreeClassifier(max_depth=5)\n","classifier_DT.fit(tfidf_train,y_train)\n","classifier_AB=AdaBoostClassifier()\n","classifier_AB.fit(tfidf_train,y_train)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1.0,\n","                   n_estimators=50, random_state=None)"]},"metadata":{"tags":[]},"execution_count":89}]},{"cell_type":"markdown","metadata":{"id":"duvirHKCYNUi"},"source":["# Detecting Fake News  "]},{"cell_type":"code","metadata":{"id":"R9D9b14yW8UH"},"source":["Australia_tweet=Countries[0]\n","#Brazil_tweet=pd.read_excel('/content/drive/Shareddrives/MY Files/Web scrapping/Data_set 1/Data.csv')\n","#China_tweet=pd.read_excel('/content/drive/Shareddrives/MY Files/Web scrapping/Data_set 1/Data.csv')\n","#Iran_tweet=pd.read_excel('/content/drive/Shareddrives/MY Files/Web scrapping/Data_set 1/Data.csv')\n","#Italy_tweet=pd.read_excel('/content/drive/Shareddrives/MY Files/Web scrapping/Data_set 1/Data.csv')\n","Singapore_tweet=Countries[1]\n","SouthKorea_tweet=Countries[2]\n","UK_tweet=Countries[3]\n","USA_tweet=Countries[4]\n","Countries_Tweet=[Australia_tweet,Singapore_tweet,SouthKorea_tweet,UK_tweet,USA_tweet]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ax2bp0ZaY2De"},"source":["def labelling_ML(df):\n","    tfidf_train1=tfidf_vectorizer.transform(df['text'])\n","    df['Passive_Aggressive']=classifier_PAC.predict(tfidf_train1)\n","    df['Logistic_Regression']=classifier_LR.predict(tfidf_train1)\n","    df['Decision_Tree']=classifier_DT.predict(tfidf_train1)\n","    df['AdaBoost']=classifier_AB.predict(tfidf_train1)\n","    df = df.dropna(how='any',axis=0)\n","    df.index=list(range(df.shape[0]))\n","    return df\n","\n","def labling_final(df):\n","   df['label']=''\n","   for i in range(df.shape[0]):\n","       n=0\n","       m=0\n","       if df['Passive_Aggressive'][i] == 'FAKE' :\n","               n=n+1\n","       else:\n","               m=m+1\n","       if df['AdaBoost'][i] == 'FAKE' :\n","               n=n+1\n","       else:\n","               m=m+1\n","       if df['Decision_Tree'][i] == 'FAKE' :\n","               n=n+1\n","       else:\n","               m=m+1\n","       if m < n:\n","               df.at[i,'label']='FAKE'    \n","       else:\n","               df.at[i,'label']='REAL'   \n","   return df\n","\n","def real_n(df):\n","   n=0\n","   for i in range(df.shape[0]):\n","        if df['label'][i]=='REAL':\n","               n=n+1\n","   return n \n","  \n","def seperation(df):\n","     fake_data=df[df['label']=='FAKE']\n","     real_data=df[df['label']=='REAL']\n","     return fake_data,real_data\n","\n","#'label_PAC','label_LR','label_DT',\"label_AB\",'\n","def Monthly_fake(fake_data):\n","   fake_data2=fake_data.copy()\n","   fake_data3=fake_data.copy()\n","   fake_data.drop(['lang','label','Passive_Aggressive','Logistic_Regression','Decision_Tree',\"AdaBoost\",'user_followers_count','user_followee_count'], axis=1, inplace=True)\n","   fake_data1 = fake_data.groupby(['time']).sum()\n","   fake_data2.drop(['lang','label','Passive_Aggressive','Logistic_Regression','Decision_Tree',\"AdaBoost\",'text'], axis=1, inplace=True)\n","   fake_data2 = fake_data2.groupby(['time']).sum()\n","   fake_data3.drop(['lang','label','Passive_Aggressive','Logistic_Regression','Decision_Tree',\"AdaBoost\",'text','user_followers_count','user_followee_count'], axis=1, inplace=True)\n","   data_tweet_frequency = fake_data3.pivot_table(index=['time'], aggfunc='size')\n","   fake_data3=fake_data3.groupby(['time']).sum()\n","   fake_data3['frequency']=data_tweet_frequency \n","   ALL_Data_fake=fake_data1.merge(fake_data2, on='time')\n","   ALL_Data_fake=ALL_Data_fake.merge(fake_data3, on='time')\n","   return ALL_Data_fake\n","\n","def Monthly_real(real_data):\n","    real_data2=real_data.copy()\n","    real_data3=real_data.copy()\n","    real_data.drop(['lang','label','Passive_Aggressive','Logistic_Regression','Decision_Tree',\"AdaBoost\",'user_followers_count','user_followee_count'], axis=1, inplace=True)\n","    real_data1 = real_data.groupby(['time']).sum()\n","    real_data2.drop(['lang','label','Passive_Aggressive','Logistic_Regression','Decision_Tree',\"AdaBoost\",'text'], axis=1, inplace=True)\n","    real_data2 = real_data2.groupby(['time']).sum()\n","    real_data3.drop(['lang','label','Passive_Aggressive','Logistic_Regression','Decision_Tree',\"AdaBoost\",'text','user_followers_count','user_followee_count'], axis=1, inplace=True)\n","    data_tweet_frequency = real_data3.pivot_table(index=['time'], aggfunc='size')\n","    real_data3=real_data3.groupby(['time']).sum()\n","    real_data3['frequency']=data_tweet_frequency \n","    ALL_Data_real=real_data1.merge(real_data2, on='time')\n","    ALL_Data_real=ALL_Data_real.merge(real_data3, on='time')\n","    return ALL_Data_real\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2DLWov6OEa49","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618108355073,"user_tz":240,"elapsed":235,"user":{"displayName":"Jaber Valinejad","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gip2ZmJmxo_rPu4BNV6ze8B_Fjb2gQ7h8I-FHgs=s64","userId":"11093588303192782745"}},"outputId":"fb0ac5de-3771-4133-b39c-6bdfd85f3a50"},"source":["#####################          Cleaning dataSET           #############################333\n","\n","import nltk\n","nltk.download('stopwords')\n","import re\n","import pandas as pd\n","import nltk\n","import string\n","from nltk.corpus import stopwords\n","#from nltk.tokenize import RegexpTokenizer\n","from nltk.tokenize import regexp\n","from nltk.stem import WordNetLemmatizer\n","from nltk.stem.porter import PorterStemmer\n","\n","\n","def remove_html(text):\n","    \"\"\"Remove special patterns - email, url, date etc.\"\"\"\n","    email_regex = re.compile(r\"[\\w.-]+@[\\w.-]+\")\n","    url_regex = re.compile(r\"(http|www)[^\\s]+\")\n","    date_regex = re.compile(r\"[\\d]{2,4}[ -/:]*[\\d]{2,4}([ -/:]*[\\d]{2,4})?\") # a way to match date\n","    ## remove\n","    text = url_regex.sub(\"\", text)\n","    text = email_regex.sub(\"\", text)\n","    text = date_regex.sub(\"\", text)\n","    return text\n","\n","\n","\n","def remove_punctuation(text):\n","    no_punct=\"\".join([c for c in text if c not in string.punctuation])\n","    return no_punct\n","\n","\n","\n","#1 No tokenize\n","#tokenized_text=result2.lower()\n","\n","#2 Tokenize\n","word_tokenizer = regexp.WhitespaceTokenizer()\n","#tokenized_text = [word.lower() for word in word_tokenizer.tokenize(result2)]\n","\n","\n","def remove_stopwords(text):\n","    words=[w for w in text if w not in stopwords.words('english')]\n","    return words\n","\n","  \n","stemmer=PorterStemmer()\n","def word_stemmer(text):\n","    stem_text=\" \".join([stemmer.stem(i) for i in text])\n","    return stem_text\n","\n","\n","def cleaning(ALL_Data_fake):  \n","  #ALL_Data_fake['texte_cleaned']=''\n","  for i in range(ALL_Data_fake.shape[0]): \n","    j=ALL_Data_fake.index[i]\n","    result=ALL_Data_fake['text'][j]\n","    result1=remove_html(result)\n","    result2=remove_punctuation(result1)\n","    tokenized_text = [word.lower() for word in word_tokenizer.tokenize(result2)]\n","    result3=remove_stopwords(tokenized_text)\n","    result4=word_stemmer(result3)\n","    ALL_Data_fake.at[j,'texte_cleaned']=result4\n","  return ALL_Data_fake"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"g7Yyv43lwQZs","executionInfo":{"status":"ok","timestamp":1618111701604,"user_tz":240,"elapsed":769873,"user":{"displayName":"Jaber Valinejad","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gip2ZmJmxo_rPu4BNV6ze8B_Fjb2gQ7h8I-FHgs=s64","userId":"11093588303192782745"}},"outputId":"836ed468-5d24-42ec-aa64-d62b552d4ea0"},"source":["def fake_process(df):\n","     df=labelling_ML(df)\n","     df=labling_final(df)\n","     fake_data,real_data=seperation(df)\n","     fake_data.index=range(fake_data.shape[0])\n","     real_data.index=range(real_data.shape[0])\n","     fake_data= Monthly_fake(fake_data)\n","     real_data=Monthly_real(real_data)\n","     fake_data= cleaning(fake_data)\n","     real_data=cleaning(real_data)\n","     return fake_data,real_data\n","\n","Countries_Tweet_fake=[Australia_tweet,Singapore_tweet,SouthKorea_tweet,UK_tweet,USA_tweet]\n","Countries_Tweet_real=[Australia_tweet,Singapore_tweet,SouthKorea_tweet,UK_tweet,USA_tweet]\n","Countries_Tweet_fake_name=['Australia_tweet_fake','Singapore_tweet_fake','SouthKorea_tweet_fake','UK_tweet_fake','USA_tweet_fake']\n","Countries_Tweet_real_name=['Australia_tweet_real','Singapore_tweet_real','SouthKorea_tweet_real','UK_tweet_real','USA_tweet_real']\n","\n","for i in range(len(Countries_Tweet)):\n","     Countries_Tweet_fake[i],Countries_Tweet_real[i]=fake_process(Countries_Tweet[i])\n","     fake=Countries_Tweet_fake[i]\n","     real=Countries_Tweet_real[i] \n","     fake_name=Countries_Tweet_fake_name[i]\n","     real_name=Countries_Tweet_real_name[i] \n","     fake.to_excel(fake_name+'.xlsx',sheet_name='Sheet_name_1') \n","     real.to_excel(real_name+'.xlsx',sheet_name='Sheet_name_1') \n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/pandas/core/frame.py:4174: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  errors=errors,\n","/usr/local/lib/python3.7/dist-packages/pandas/core/frame.py:4174: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  errors=errors,\n","/usr/local/lib/python3.7/dist-packages/pandas/core/frame.py:4174: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  errors=errors,\n","/usr/local/lib/python3.7/dist-packages/pandas/core/frame.py:4174: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  errors=errors,\n","/usr/local/lib/python3.7/dist-packages/pandas/core/frame.py:4174: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  errors=errors,\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"Al3kAL2wGAhJ"},"source":["Countries_Tweet_fake[0].index=range(Countries_Tweet_fake[0].shape[0])\n","y=Countries_Tweet_fake[0]\n","#np.savetxt(r'c:\\data\\np.txt', np.array(y['texte_cleaned'][0]).value, fmt='%d')\n","#Countries_Tweet_fake[0].to_string('fff.txt') "],"execution_count":null,"outputs":[]}]}